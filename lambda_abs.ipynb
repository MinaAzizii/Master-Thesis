{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinaAzizii/Master-Thesis/blob/main/lambda_abs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V7d3SGdQzgc"
      },
      "outputs": [],
      "source": [
        "!pip uninstall torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9yU_x8PQ7HN"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # GPU version\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMDSIDciQ9tS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(\"✅ CUDA is available!\")\n",
        "else:\n",
        "    print(\"❌ CUDA is not available. Check your PyTorch installation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSAlLYYNRAtk"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade numpy==1.24.4 # Ensure numpy is upgraded as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSz4UwymRDfJ"
      },
      "outputs": [],
      "source": [
        "!pip install pandas openpyxl seaborn matplotlib tqdm scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixghmZw_RGdg"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit-pypi scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSE43e3URJG0"
      },
      "outputs": [],
      "source": [
        "!pip install networkx==2.8.8\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpIOwpKERLbf"
      },
      "outputs": [],
      "source": [
        "!pip install mordred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rISeZShMRN-V"
      },
      "outputs": [],
      "source": [
        "# Import PyTorch modules for neural network creation and optimization\n",
        "import torch\n",
        "import torch.nn as nn  # Neural network building blocks (layers, activations)\n",
        "import torch.optim as optim  # Optimization algorithms (Adam)\n",
        "\n",
        "# Import PyTorch utilities for dataset management\n",
        "from torch.utils.data import Dataset  # Base class for custom dataset implementation\n",
        "\n",
        "# Import PyTorch Geometric for graph-based neural networks\n",
        "from torch_geometric.loader import DataLoader  # Efficient batch loading for graph data\n",
        "from torch_geometric.data import Data, Batch  # Graph data structures\n",
        "from torch_geometric.nn import NNConv, global_mean_pool  # Graph neural network layers (NNConv) and pooling function\n",
        "\n",
        "# RDKit for chemistry-related operations (SMILES parsing, molecular descriptors)\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "from rdkit.Chem import SanitizeMol, SanitizeFlags\n",
        "# Pandas and NumPy for data handling and manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Scikit-learn utilities for data preprocessing and model evaluation\n",
        "from sklearn.preprocessing import StandardScaler  # Feature standardization\n",
        "from sklearn.metrics import r2_score, mean_squared_error  # Evaluation metrics ( MSE..)\n",
        "from sklearn.model_selection import train_test_split  # Dataset splitting\n",
        "\n",
        "# Visualization libraries for plotting model results and performance\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Random module for reproducibility and randomness control\n",
        "import random\n",
        "\n",
        "# Suppress unnecessary RDKit warnings for cleaner output\n",
        "from rdkit import RDLogger\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KiTf7RWIfOU"
      },
      "outputs": [],
      "source": [
        "# Disable unnecessary RDKit logging to avoid excessive warnings during molecule processing\n",
        "RDLogger.DisableLog('rdApp.*')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0KLcujH64fj"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility across PyTorch , NumPy..\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHBilE_XAcqm"
      },
      "outputs": [],
      "source": [
        "# Computes molecular descriptors for a given SMILES string.\n",
        "def compute_descriptors(smiles, func_list):\n",
        "    m = Chem.MolFromSmiles(smiles)\n",
        "    vals = []\n",
        "    for _, fn in func_list:\n",
        "        try:\n",
        "            v = fn(m)\n",
        "            vals.append(v if np.isfinite(v) else 0.0)\n",
        "        except:\n",
        "            vals.append(0.0)\n",
        "    return np.array(vals, dtype=float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ6r4FqTqZet"
      },
      "outputs": [],
      "source": [
        "# 1) LOAD & CLEAN\n",
        "df = pd.read_excel(\"DB2.xlsx\")\n",
        "\n",
        "df = df.dropna(subset=['smiles', 'solvent', 'abs', 'solvent_name'])\n",
        "mask_valid = (\n",
        "    df['smiles'].apply(lambda s: Chem.MolFromSmiles(s) is not None) &\n",
        "    df['solvent'].apply(lambda s: Chem.MolFromSmiles(s) is not None)\n",
        ")\n",
        "df = df[mask_valid].reset_index(drop=True)\n",
        "print(f\"Dropped {len(df) - len(df)} rows with invalid SMILES or missing 'abs'.\")\n",
        "\n",
        "print(f\"Number of samples with valid λ-absorption: {len(df)}\")\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")  # (rows, columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ9J58YOCtHr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Check SMILES for unusual charges and optionally remove problematic entries\n",
        "\n",
        "bad = []  # list of (column, row_index, smi)\n",
        "for col in ['smiles','solvent']:\n",
        "    for i, smi in enumerate(df[col]):\n",
        "        m = Chem.MolFromSmiles(smi, sanitize=False)\n",
        "        code = Chem.SanitizeMol(m,\n",
        "                                sanitizeOps=SanitizeFlags.SANITIZE_PROPERTIES,\n",
        "                                catchErrors=True)\n",
        "        if code != Chem.SanitizeFlags.SANITIZE_NONE:\n",
        "            bad.append((col, i, smi))\n",
        "\n",
        "if bad:\n",
        "    print(\"⚠️ Found unusual‐charge warnings in these rows:\")\n",
        "    for col, i, smi in bad:\n",
        "        print(f\" • {col} @ row {i}: {smi}\")\n",
        "    # optionally drop them:\n",
        "    drop_idxs = {i for _, i, _ in bad}\n",
        "    df = df.drop(drop_idxs).reset_index(drop=True)\n",
        "    print(f\"Dropped {len(drop_idxs)} rows.\")\n",
        "else:\n",
        "    print(\"No unusual‐charge warnings detected.\")\n",
        "\n",
        "print(f\"Remaining samples: {len(df)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfU0oEoSCIhr"
      },
      "outputs": [],
      "source": [
        "# --- validate SMILES for both molecule and solvent ---\n",
        "is_valid_mol = df['smiles'].apply(lambda s: Chem.MolFromSmiles(s) is not None)\n",
        "is_valid_sol = df['solvent'].apply(lambda s: Chem.MolFromSmiles(s) is not None)\n",
        "mask_valid   = is_valid_mol & is_valid_sol\n",
        "\n",
        "# count and remove invalid rows\n",
        "n_invalid = (~mask_valid).sum()\n",
        "df        = df[mask_valid].reset_index(drop=True)\n",
        "print(f\"Removed {n_invalid} rows with invalid SMILES.\")\n",
        "\n",
        "# report how many samples remain\n",
        "print(f\"Remaining samples (with valid 'abs'): {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bQ3-B_VBngQ"
      },
      "outputs": [],
      "source": [
        "# 2) DESCRIPTORS\n",
        "mol_funcs = [\n",
        "    (\"Mol_MolWt\", Descriptors.MolWt),\n",
        "    (\"Mol_TPSA\", Descriptors.TPSA),\n",
        "    (\"Mol_NumRotatableBonds\", Descriptors.NumRotatableBonds),\n",
        "    (\"Mol_LogP\", Descriptors.MolLogP),\n",
        "    (\"Mol_Aromaticity\", Descriptors.NumAromaticRings),\n",
        "    (\"Mol_NumHDonors\", Descriptors.NumHDonors),\n",
        "    (\"Mol_NumHAcceptors\", Descriptors.NumHAcceptors),\n",
        "    (\"Mol_FractionCSP3\", Descriptors.FractionCSP3),\n",
        "    (\"Mol_HeteroatomCount\", Descriptors.HeavyAtomCount),\n",
        "]\n",
        "solvent_funcs = [\n",
        "    (\"Solv_MolWt\", Descriptors.MolWt),\n",
        "    (\"Solv_TPSA\", Descriptors.TPSA),\n",
        "    (\"Solv_MolLogP\", Descriptors.MolLogP),\n",
        "    (\"Solv_NumHDonors\", Descriptors.NumHDonors),\n",
        "]\n",
        "\n",
        " # Precompute descriptor arrays for unique molecules and solvents\n",
        "mol_raw = {sm: compute_descriptors(sm, mol_funcs) for sm in df['smiles'].unique()}\n",
        "solv_raw = {sm: compute_descriptors(sm, solvent_funcs) for sm in df['solvent'].unique()}\n",
        "\n",
        "mol_arr = np.vstack([mol_raw[sm] for sm in df['smiles']])\n",
        "solv_arr = np.vstack([solv_raw[sm] for sm in df['solvent']])\n",
        "\n",
        "desc_df = pd.DataFrame({\n",
        "    'smiles': df['smiles'],\n",
        "    'solvent': df['solvent'],\n",
        "    'abs': df['abs'],\n",
        "    'solvent_name': df['solvent_name']\n",
        "})\n",
        "for i, (col, _) in enumerate(solvent_funcs):\n",
        "    desc_df[col] = solv_arr[:, i]\n",
        "for i, (col, _) in enumerate(mol_funcs):\n",
        "    desc_df[col] = mol_arr[:, i]\n",
        "desc_df.to_excel(\"all_raw_descriptors.xlsx\", index=False)\n",
        "print(\"✅ Saved all_raw_descriptors.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1s1y8dnCrP-"
      },
      "outputs": [],
      "source": [
        "# 3) SPLIT & SCALE\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "solv_scaler   = StandardScaler().fit(solv_arr)\n",
        "mol_scaler    = StandardScaler().fit(mol_arr)\n",
        "target_scaler = StandardScaler().fit(train_df[['abs']].values)\n",
        "\n",
        "solv_scaled = {sm: torch.tensor(solv_scaler.transform(raw.reshape(1, -1))[0], dtype=torch.float)\n",
        "               for sm, raw in solv_raw.items()}\n",
        "mol_scaled  = {sm: torch.tensor(mol_scaler.transform(raw.reshape(1, -1))[0], dtype=torch.float)\n",
        "               for sm, raw in mol_raw.items()}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVE SCALERS:\n",
        "import pickle\n",
        "\n",
        "with open(\"solv_scaler.pkl\", \"wb\") as f:\n",
        "    pickle.dump(solv_scaler, f)\n",
        "\n",
        "with open(\"mol_scaler.pkl\", \"wb\") as f:\n",
        "    pickle.dump(mol_scaler, f)\n",
        "\n",
        "with open(\"target_scaler.pkl\", \"wb\") as f:\n",
        "    pickle.dump(target_scaler, f)"
      ],
      "metadata": {
        "id": "WHzCpVPfIvzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plB6Se7pAQRV"
      },
      "outputs": [],
      "source": [
        "# SMILES to graph conversion\n",
        "def mol_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    atoms = mol.GetAtoms()\n",
        "    heavy = [i for i, a in enumerate(atoms) if a.GetAtomicNum() > 1] or list(range(len(atoms)))\n",
        "    idx_map = {old: i for i, old in enumerate(heavy)}\n",
        "    x = torch.tensor([[atoms[i].GetAtomicNum(), atoms[i].GetFormalCharge(), atoms[i].GetNumExplicitHs()]\n",
        "                      for i in heavy], dtype=torch.float)\n",
        "    edges, attrs = [], []\n",
        "    for b in mol.GetBonds():\n",
        "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
        "        if i in heavy and j in heavy:\n",
        "            ei, ej = idx_map[i], idx_map[j]\n",
        "            onehot = [int(b.GetBondType() == t) for t in\n",
        "                      (Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE,\n",
        "                       Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC)]\n",
        "            edges += [[ei, ej], [ej, ei]]\n",
        "            attrs += [onehot, onehot]\n",
        "    if not edges:\n",
        "        edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
        "        edge_attr  = torch.zeros((0, 4), dtype=torch.float)\n",
        "    else:\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "        edge_attr  = torch.tensor(attrs, dtype=torch.float)\n",
        "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EDK8fXBAXWQ"
      },
      "outputs": [],
      "source": [
        "# Custom dataset class for molecular absorption data\n",
        "# -Converts SMILES to graph data (molecule & solvent)\n",
        "class AbsDataset(Dataset):\n",
        "    def __init__(self, df): self.df = df.reset_index(drop=True)\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        row = self.df.iloc[i]\n",
        "        mg = mol_to_graph(row['smiles'])\n",
        "        sg = mol_to_graph(row['solvent'])\n",
        "        sdesc = solv_scaled[row['solvent']]\n",
        "        mdesc = mol_scaled[row['smiles']]\n",
        "        y = target_scaler.transform([[row['abs']]])[0, 0]\n",
        "        return mg, sg, sdesc, mdesc, torch.tensor(y, dtype=torch.float), row['solvent_name']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uMsnqGvAc5j"
      },
      "outputs": [],
      "source": [
        "# Batch molecular and solvent graphs using PyTorch geometric’s batch.\n",
        "# Stack scaled descriptors and target values.\n",
        "# Prepare training and validation loaders with defined batch sizes.\n",
        "\n",
        "def collate_fn(batch):\n",
        "    mg, sg, sd, md, y, names = zip(*batch)\n",
        "    return (Batch.from_data_list(mg), Batch.from_data_list(sg),\n",
        "            torch.stack(sd), torch.stack(md), torch.stack(y), list(names))\n",
        "\n",
        "train_loader = DataLoader(AbsDataset(train_df), batch_size=32, shuffle=True,  collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(AbsDataset(val_df),   batch_size=64, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rFI_x07tUd5"
      },
      "outputs": [],
      "source": [
        "# GRAPH CONVOLUTIONAL NETWORK ENCODER:\n",
        "# Defines a GCN encoder using NNConv layers with edge-conditioned convolution.\n",
        "# Uses three graph convolutional layers with ReLU activations.\n",
        "# Employs distinct edge networks for computing edge-dependent weights.\n",
        "# Aggregates node embeddings into a global graph representation via mean pooling.\n",
        "\n",
        "\n",
        "class GCNEncoder(nn.Module):\n",
        "    def __init__(self, in_dim, hid, out_dim):\n",
        "        super().__init__()\n",
        "        # edge networks\n",
        "        self.e1 = nn.Sequential(nn.Linear(4, hid * in_dim), nn.ReLU(), nn.Linear(hid * in_dim, hid * in_dim))\n",
        "        self.e2 = nn.Sequential(nn.Linear(4, hid * hid), nn.ReLU(), nn.Linear(hid * hid, hid * hid))\n",
        "        self.e3 = nn.Sequential(nn.Linear(4, out_dim * hid), nn.ReLU(), nn.Linear(out_dim * hid, out_dim * hid))\n",
        "\n",
        "        # GCN layers\n",
        "        self.c1 = NNConv(in_dim, hid, self.e1, aggr='mean')\n",
        "        self.c2 = NNConv(hid, hid, self.e2, aggr='mean')\n",
        "        self.c3 = NNConv(hid, out_dim, self.e3, aggr='mean')\n",
        "\n",
        "    def forward(self, x, ei, batch, ea):\n",
        "        x = self.c1(x, ei, ea).relu()\n",
        "        x = self.c2(x, ei, ea).relu()\n",
        "        x = self.c3(x, ei, ea).relu()\n",
        "        return global_mean_pool(x, batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHagiUINrrny"
      },
      "outputs": [],
      "source": [
        "class SolvationPredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.gcn_mol = GCNEncoder(3, 64, 64)  # Input dim, hidden dim, output dim for molecule GCN\n",
        "        self.gcn_sol = GCNEncoder(3, 32, 64)  # Input dim, hidden dim, output dim for solvent GCN\n",
        "        self.mlp_sol = nn.Sequential(nn.Linear(4, 64), nn.ReLU(), nn.Linear(64, 64))\n",
        "        self.mlp_mol = nn.Sequential(nn.Linear(9, 64), nn.ReLU(), nn.Linear(64, 64))\n",
        "\n",
        "        # MLP for processing concatenated GCN outputs\n",
        "        self.mlp_gcn = nn.Sequential(\n",
        "            nn.Linear(64 + 64, 128),  # Input size is now the sum of GCN output sizes (Adjusted)\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Second MLP to fuse GCN-derived features with descriptor features\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(64 + 64 + 64, 128),  # Input size: GCN output + solvent descriptors + molecule descriptors\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 1)  # Output a single value for lambda absorption\n",
        "        )\n",
        "\n",
        "    def forward(self, mg, sg, sdesc, mdesc):\n",
        "        # Get the features from the GCNs\n",
        "        me = self.gcn_mol(mg.x, mg.edge_index, mg.batch, mg.edge_attr)\n",
        "        se = self.gcn_sol(sg.x, sg.edge_index, sg.batch, sg.edge_attr)\n",
        "\n",
        "        # Pass descriptors through MLPs\n",
        "        sf = self.mlp_sol(sdesc)\n",
        "        mf = self.mlp_mol(mdesc)\n",
        "\n",
        "        # Concatenate GCN outputs\n",
        "        gcn_cat = torch.cat([me, se], dim=-1)\n",
        "\n",
        "        # Pass concatenated GCN outputs through the MLP\n",
        "        gcn_out = self.mlp_gcn(gcn_cat)\n",
        "\n",
        "        # Concatenate with descriptor features for final prediction\n",
        "        cat = torch.cat([gcn_out, sf, mf], dim=-1)  # Includes descriptor features\n",
        "\n",
        "        return self.fuse(cat).squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOx5zoHMAkNz"
      },
      "outputs": [],
      "source": [
        "# MODEL SETUP:\n",
        "# Select GPU (cuda) if available, otherwise CPU.\n",
        "# Initialize the SolvationPredictor model on selected device.\n",
        "# Use Adam optimizer with learning rate 1e-4 and weight decay for regularization.\n",
        "#Employ SmoothL1Loss as the loss function for regression training.\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model  = SolvationPredictor().to(device)\n",
        "opt    = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-7)\n",
        "loss_fn= nn.SmoothL1Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSgy2WTZcvxL"
      },
      "outputs": [],
      "source": [
        " #MODEL TRAINING LOOP:\n",
        "# Train for 100 epochs, evaluating after each epoch.\n",
        "#  Computes SmoothL1Loss for both training and validation datasets.\n",
        "#  Records average losses per epoch.\n",
        "#  Prints training and validation loss per epoch.\n",
        "train_losses, val_losses = [], []\n",
        "for ep in range(1, 101):\n",
        "    model.train()\n",
        "    t_loss, t_cnt = 0., 0\n",
        "    for mg, sg, sd, md, y, _ in train_loader:\n",
        "        mg, sg, sd, md, y = mg.to(device), sg.to(device), sd.to(device), md.to(device), y.to(device)\n",
        "        pred = model(mg, sg, sd, md)\n",
        "        loss = loss_fn(pred, y)\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "        t_loss += loss.item() * y.size(0); t_cnt += y.size(0)\n",
        "    train_losses.append(t_loss/t_cnt)\n",
        "\n",
        "    model.eval()\n",
        "    v_loss, v_cnt = 0., 0\n",
        "    with torch.no_grad():\n",
        "        for mg, sg, sd, md, y, _ in val_loader:\n",
        "            mg, sg, sd, md, y = mg.to(device), sg.to(device), sd.to(device), md.to(device), y.to(device)\n",
        "            pred = model(mg, sg, sd, md)\n",
        "            loss = loss_fn(pred, y)\n",
        "            v_loss += loss.item() * y.size(0); v_cnt += y.size(0)\n",
        "    val_losses.append(v_loss/v_cnt)\n",
        "    print(f\"Epoch {ep:02d} → Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr3vpXJuAp_f"
      },
      "outputs": [],
      "source": [
        "# 8) EVALUATION\n",
        "model.eval()\n",
        "all_pred, all_true, all_names = [], [], []\n",
        "with torch.no_grad():\n",
        "    for mg, sg, sd, md, y, names in val_loader:\n",
        "        mg, sg, sd, md = mg.to(device), sg.to(device), sd.to(device), md.to(device)\n",
        "        pred = model(mg, sg, sd, md).cpu().numpy()\n",
        "        all_pred.append(pred)\n",
        "        all_true.append(y.numpy())\n",
        "        all_names += names\n",
        "\n",
        "# concatenate batched outputs\n",
        "pred_scaled = np.concatenate(all_pred)\n",
        "true_scaled = np.concatenate(all_true)\n",
        "\n",
        "# compute scaled‐range metrics\n",
        "r2_scaled = r2_score(true_scaled, pred_scaled)\n",
        "mse_scaled = mean_squared_error(true_scaled, pred_scaled)\n",
        "rmse_scaled = np.sqrt(mse_scaled)  # Calculate RMSE\n",
        "\n",
        "# Print results for scaled values\n",
        "print(f\"R² (scaled) = {r2_scaled:.3f}\")\n",
        "print(f\"MSE (scaled) = {mse_scaled:.3f}\")\n",
        "print(f\"RMSE (scaled) = {rmse_scaled:.3f}\")  # Print RMSE (scaled)\n",
        "\n",
        "# inverse‐transform to real units\n",
        "pred_real = target_scaler.inverse_transform(pred_scaled.reshape(-1,1)).reshape(-1)\n",
        "true_real = target_scaler.inverse_transform(true_scaled.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "# compute real‐range metrics\n",
        "r2_real = r2_score(true_real, pred_real)\n",
        "mse_real = mean_squared_error(true_real, pred_real)\n",
        "rmse_real = np.sqrt(mse_real)  # Calculate RMSE\n",
        "\n",
        "# Print results for real values\n",
        "print(f\"R² (real)   = {r2_real:.3f}\")\n",
        "print(f\"MSE (real)  = {mse_real:.3f}\")\n",
        "print(f\"RMSE (real) = {rmse_real:.3f}\")  # Print RMSE (real)\n",
        "\n",
        "# 9) PLOTTING\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# scatter in scaled space\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(true_scaled, pred_scaled, alpha=0.6)\n",
        "mn, mx = min(true_scaled.min(), pred_scaled.min()), max(true_scaled.max(), pred_scaled.max())\n",
        "plt.plot([mn, mx], [mn, mx], 'k--')\n",
        "plt.xlabel(\"Actual (scaled)\")\n",
        "plt.ylabel(\"Predicted (scaled)\")\n",
        "plt.title(\"Pred vs. Actual (scaled)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# scatter in real space\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(true_real, pred_real, alpha=0.6)\n",
        "mn, mx = min(true_real.min(), pred_real.min()), max(true_real.max(), pred_real.max())\n",
        "plt.plot([mn, mx], [mn, mx], 'k--')\n",
        "plt.xlabel(\"Actual (real)\")\n",
        "plt.ylabel(\"Predicted (real)\")\n",
        "plt.title(\"Pred vs. Actual (real)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k4IsL-YJW1G"
      },
      "outputs": [],
      "source": [
        "# 9) EXPORT & PLOTTING\n",
        "res_df = pd.DataFrame({\n",
        "    'solvent_name':          all_names,\n",
        "    'actual_absorption':     true_real,\n",
        "    'predicted_absorption':  pred_real\n",
        "})\n",
        "res_df.to_excel(\"absorption_predictions.xlsx\", index=False)\n",
        "print(\"✅ Saved absorption_predictions.xlsx\")\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(range(1,101), train_losses, label='Train')\n",
        "plt.plot(range(1,101), val_losses,   label='Val')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
        "plt.title('Training vs. Validation Loss'); plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(true_real, pred_real, alpha=0.6)\n",
        "mn, mx = min(true_real.min(), pred_real.min()), max(true_real.max(), pred_real.max())\n",
        "plt.plot([mn, mx], [mn, mx], 'k--')\n",
        "plt.xlabel('Actual Absorption'); plt.ylabel('Predicted Absorption')\n",
        "plt.title('Pred vs. Actual'); plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7_5VmnpAts7"
      },
      "outputs": [],
      "source": [
        "#Save model\n",
        "torch.save(model.state_dict(), \"model_abs.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"model_abs.pt\")\n"
      ],
      "metadata": {
        "id": "3g6x3hZKu0wm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMpA2LtURSs+4qF07EsrRaz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}